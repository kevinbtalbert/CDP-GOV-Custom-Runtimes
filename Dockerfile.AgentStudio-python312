# Cloudera AI Agent Studio Custom Runtime for GovCloud
#
# This Dockerfile builds Agent Studio by cloning the source from GitHub.
# No need to manually clone the repository - it's done during the build.
#
# Build command:
#   docker build -f Dockerfile.AgentStudio-python312 -t your-image-name:tag .
#
# To build a specific version/tag/branch:
#   docker build --build-arg AGENT_STUDIO_VERSION=v2.0.0 -f Dockerfile.AgentStudio-python312 -t your-image-name:tag .
#
# Source: https://github.com/cloudera/CAI_STUDIO_AGENT
#
FROM --platform=linux/amd64 container-public.repo.cdp.clouderagovt.com/cloudera-public/cloudera/cdsw/ml-runtime-pbj-workbench-python3.12-govcloud:2025.07.1-b6 AS builder

USER root
WORKDIR /build

# Note: Base image has no package manager (apt/yum/dnf/microdnf)
# Assuming required tools (curl, gcc, make, git) are pre-installed in the Cloudera ML Runtime
# If tools are missing, build will fail at the step that needs them

# Clone Agent Studio source code from GitHub
# You can override this version with --build-arg AGENT_STUDIO_VERSION=<tag/branch>
ARG AGENT_STUDIO_VERSION=main
RUN git clone --depth 1 --branch ${AGENT_STUDIO_VERSION} https://github.com/cloudera/CAI_STUDIO_AGENT.git /studio_app

# Set working directory to cloned source
WORKDIR /studio_app

# Force Python 3.12 usage by overwriting .python-version
# This prevents uv from downloading Python 3.10 at runtime
RUN echo "3.12" > .python-version && \
    echo "3.12" > studio/workflow_engine/.python-version

# Patch workbench.py to prioritize Python 3.12 in the runtime search path
# Original: python_versions = ["python3.10", "python3.11", "python3.9"]
# Updated: python_versions = ["python3.12", "python3.10", "python3.11", "python3.9"]
RUN sed -i 's/python_versions = \["python3\.10", "python3\.11", "python3\.9"\]/python_versions = ["python3.12", "python3.10", "python3.11", "python3.9"]/' \
    studio/workflow_engine/src/engine/entry/workbench.py

# Install uv
RUN pip install uv==0.5.29

# Use system Python 3.12 (FIPS-compliant) for virtual environments
# The base image already has Python 3.12 with FIPS OpenSSL support
RUN python3.12 --version && \
    python3.12 -c "import sys; print(f'Python: {sys.version}')" && \
    python3.12 -c "import ssl; print(f'OpenSSL: {ssl.OPENSSL_VERSION}')"

# Install NVM and Node.js 22
ENV NVM_DIR="/studio_app/.nvm"
RUN mkdir -p "$NVM_DIR"

# Download and install NVM
RUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash

# Install Node.js 22 using NVM
# Note: BusyBox 'ls' doesn't support -q flag, so nvm use/alias fail
# We install node and reference it directly by version path
RUN . "$NVM_DIR/nvm.sh" && nvm install 22

# Create symlinks to make node and npm available globally
# Directly reference the installed version (v22.x.x) since nvm use fails with BusyBox
RUN NODE_VERSION=$(ls "$NVM_DIR/versions/node" | grep "^v22" | head -n 1) && \
    ln -sf "$NVM_DIR/versions/node/$NODE_VERSION/bin/node" /usr/local/bin/node && \
    ln -sf "$NVM_DIR/versions/node/$NODE_VERSION/bin/npm" /usr/local/bin/npm && \
    ln -sf "$NVM_DIR/versions/node/$NODE_VERSION/bin/npx" /usr/local/bin/npx && \
    echo "Installed Node.js version: $NODE_VERSION"

# Install Python dependencies using system FIPS-compliant Python 3.12
# Note: Force uv to use Python 3.12 even though pyproject.toml specifies Python 3.10
ENV UV_PYTHON_PREFERENCE=only-system
RUN uv venv --python python3.12 && \
    uv sync --no-dev --python python3.12

# Install Node.js dependencies
RUN npm install

# Build Next.js application
# Set Node.js options to reduce memory pressure and avoid QEMU segfaults during cross-compilation
# Disable Next.js telemetry and reduce parallelism to avoid QEMU crashes
RUN NEXT_TELEMETRY_DISABLED=1 \
    NODE_OPTIONS="--max-old-space-size=2048" \
    npm run build -- --no-lint

# Clean up build artifacts
RUN rm -rf node_modules && npm cache clean --force

# Create workflow engine virtual environment using system Python 3.12
RUN cd studio/workflow_engine && \
    uv venv --python python3.12 && \
    VIRTUAL_ENV=.venv uv sync --no-dev --python python3.12

# Stage 2: Runtime stage
FROM --platform=linux/amd64 container-public.repo.cdp.clouderagovt.com/cloudera-public/cloudera/cdsw/ml-runtime-pbj-workbench-python3.12-govcloud:2025.07.1-b6

USER root
RUN mkdir -p /studio_app && chown cdsw:cdsw /studio_app
WORKDIR /studio_app

# Note: Base image has no package manager - assuming curl is pre-installed
# Install only uv in runtime
RUN pip install uv==0.5.29

# Note: The Ubuntu 24 CVE fixes are not applicable to this Red Hat UBI8 base image
# The base image has no package manager available, so package removal for CVEs
# must be handled by using a different base image or contacting image maintainer

# Copy everything from builder
COPY --from=builder /studio_app /studio_app

# Force uv to use system Python 3.12 at runtime (prevents downloading Python 3.10)
# Set explicit database path to avoid any path resolution issues
ENV UV_PYTHON_PREFERENCE=only-system \
    UV_PYTHON=python3.12 \
    AGENT_STUDIO_SQLITE_DB=/home/cdsw/agent-studio/.app/state.db

# Set ownership and permissions for cdsw user
RUN chown -R cdsw:cdsw /studio_app && \
    chmod -R 755 /studio_app/.venv /studio_app/studio/workflow_engine/.venv && \
    test -d /studio_app/bin && chmod -R 755 /studio_app/bin || true && \
    test -d /studio_app/studio/bin && chmod -R 755 /studio_app/studio/bin || true && \
    echo "Verifying venv permissions and FIPS Python 3.12:" && \
    ls -la /studio_app/.venv/bin/python* && \
    ls -la /studio_app/studio/workflow_engine/.venv/bin/python* && \
    echo "Testing Python access and SSL:" && \
    su -c "/studio_app/.venv/bin/python3 --version" cdsw && \
    su -c "/studio_app/.venv/bin/python3 -c 'import ssl; print(\"SSL:\", ssl.OPENSSL_VERSION)'" cdsw

# Remove what we don't need in runtime and do minimal permission fixes
RUN rm -rf \
        /studio_app/.git \
        /studio_app/docs \
        /studio_app/tests \
        /studio_app/.pytest_cache \
        /studio_app/__pycache__ \
        /studio_app/node_modules \
        /studio_app/eslint.config.mjs \
        /studio_app/tailwind.config.js \
        /studio_app/tsconfig.json \
        /studio_app/postcss.config.js \
        /studio_app/components.json \
        /studio_app/.prettierrc \
        /studio_app/next.config.ts

# Set NVM environment variable (copied from builder stage)
ENV NVM_DIR="/studio_app/.nvm"

# Recreate Node.js symlinks and setup NVM for runtime
RUN NODE_VERSION=$(ls "$NVM_DIR/versions/node" | grep "^v22" | head -n 1) && \
    ln -sf "$NVM_DIR/versions/node/$NODE_VERSION/bin/node" /usr/local/bin/node && \
    ln -sf "$NVM_DIR/versions/node/$NODE_VERSION/bin/npm" /usr/local/bin/npm && \
    ln -sf "$NVM_DIR/versions/node/$NODE_VERSION/bin/npx" /usr/local/bin/npx && \
    echo "Node.js runtime version: $NODE_VERSION" && \
    echo "export NVM_DIR=\"$NVM_DIR\"" > /etc/profile.d/nvm.sh && \
    echo "[ -s \"\$NVM_DIR/nvm.sh\" ] && . \"\$NVM_DIR/nvm.sh\"" >> /etc/profile.d/nvm.sh && \
    echo "export PATH=\"\$NVM_DIR/versions/node/$NODE_VERSION/bin:\$PATH\"" >> /etc/profile.d/nvm.sh && \
    chmod 755 /etc/profile.d/nvm.sh && \
    echo "source /etc/profile.d/nvm.sh" >> /etc/bash.bashrc && \
    echo "export NVM_DIR=\"$NVM_DIR\"" >> /home/cdsw/.bashrc && \
    echo "[ -s \"\$NVM_DIR/nvm.sh\" ] && . \"\$NVM_DIR/nvm.sh\"" >> /home/cdsw/.bashrc && \
    echo "export PATH=\"\$NVM_DIR/versions/node/$NODE_VERSION/bin:\$PATH\"" >> /home/cdsw/.bashrc && \
    chown cdsw:cdsw /home/cdsw/.bashrc

# Install production Node.js dependencies using the copied NVM from builder
RUN npm ci --only=production && npm cache clean --force

# Create a wrapper script for nvm command to ensure it's always sourced
RUN echo '#!/bin/bash' > /usr/local/bin/nvm && \
    echo 'export NVM_DIR="/studio_app/.nvm"' >> /usr/local/bin/nvm && \
    echo '[ -s "$NVM_DIR/nvm.sh" ] && . "$NVM_DIR/nvm.sh"' >> /usr/local/bin/nvm && \
    echo 'nvm "$@"' >> /usr/local/bin/nvm && \
    chmod +x /usr/local/bin/nvm

# Set environment variables
ENV AGENT_STUDIO_DEPLOY_MODE=runtime
ENV IS_COMPOSABLE=true
ENV APP_DIR=/studio_app
ENV APP_DATA_DIR=/home/cdsw/agent-studio

# Update PATH to include venv and node binaries
# We dynamically determine the Node.js version path at build time
RUN NODE_VERSION=$(ls "$NVM_DIR/versions/node" | grep "^v22" | head -n 1) && \
    echo "export PATH=\"/studio_app/.venv/bin:$NVM_DIR/versions/node/$NODE_VERSION/bin:/usr/local/bin:\$PATH\"" >> /etc/profile.d/agent-studio-path.sh && \
    chmod 755 /etc/profile.d/agent-studio-path.sh

ENV PATH="/studio_app/.venv/bin:/usr/local/bin:$PATH"

ENV ML_RUNTIME_EDITION="Agent Studio for GovCloud" \
    ML_RUNTIME_DESCRIPTION="Agent Studio runtime provided by Cloudera" \
    ML_RUNTIME_KERNEL="Agent Studio" \
    ML_RUNTIME_EDITOR="JupyterLab" \
    ML_RUNTIME_METADATA_VERSION=2 \ 
    ML_RUNTIME_JUPYTER_KERNEL_NAME="python3" \
    ML_RUNTIME_FULL_VERSION="2.0.0.71" \
    ML_RUNTIME_SHORT_VERSION="2.0.0" \
    ML_RUNTIME_MAINTENANCE_VERSION=71 \
    ML_RUNTIME_GIT_HASH=$"8464d17af7a8a1e0875de65b4e9a7a41deef1cff" \
    ML_RUNTIME_GBN=71591108 \
    JUPYTERLAB_WORKSPACES_DIR=/tmp

LABEL \
    com.cloudera.ml.runtime.runtime-metadata-version=$ML_RUNTIME_METADATA_VERSION \
    com.cloudera.ml.runtime.edition=$ML_RUNTIME_EDITION \
    com.cloudera.ml.runtime.description=$ML_RUNTIME_DESCRIPTION \
    com.cloudera.ml.runtime.editor=$ML_RUNTIME_EDITOR \
    com.cloudera.ml.runtime.full-version=$ML_RUNTIME_FULL_VERSION \
    com.cloudera.ml.runtime.short-version=$ML_RUNTIME_SHORT_VERSION \
    com.cloudera.ml.runtime.kernel=$ML_RUNTIME_KERNEL \
    com.cloudera.ml.runtime.maintenance-version=$ML_RUNTIME_MAINTENANCE_VERSION \
    com.cloudera.ml.runtime.git-hash=$ML_RUNTIME_GIT_HASH \
    com.cloudera.ml.runtime.gbn=$ML_RUNTIME_GBN
    

COPY ml-runtime-editor /usr/local/bin/ml-runtime-editor
RUN chmod +x /usr/local/bin/ml-runtime-editor

RUN pip3 install jupyterlab

# Ensure cdsw user home directory has proper permissions
# Note: /home/cdsw/agent-studio appears to be a mounted volume in CAI,
# so we create a startup script to initialize symlinks and directories at runtime
RUN mkdir -p /home/cdsw && \
    chown -R cdsw:cdsw /home/cdsw && \
    chmod -R 755 /home/cdsw

# Create initialization script that runs at container startup
RUN mkdir -p /usr/local/bin && \
    cat > /usr/local/bin/init-agent-studio-dirs.sh << 'INIT_SCRIPT'
#!/bin/bash
# Initialize Agent Studio directories and symlinks
# This runs at startup because /home/cdsw/agent-studio is a mounted volume

# Create .app directory for SQLite database
mkdir -p /home/cdsw/agent-studio/.app
chmod 755 /home/cdsw/agent-studio/.app

# Create symlinks to /studio_app resources (if they don't exist)
[ ! -e /home/cdsw/agent-studio/bin ] && ln -s /studio_app/bin /home/cdsw/agent-studio/bin
[ ! -e /home/cdsw/agent-studio/alembic ] && ln -s /studio_app/alembic /home/cdsw/agent-studio/alembic
[ ! -e /home/cdsw/agent-studio/.venv ] && ln -s /studio_app/.venv /home/cdsw/agent-studio/.venv
[ ! -e /home/cdsw/agent-studio/studio ] && ln -s /studio_app/studio /home/cdsw/agent-studio/studio

exit 0
INIT_SCRIPT
RUN chmod +x /usr/local/bin/init-agent-studio-dirs.sh

# Patch the Python startup script to call our initialization script
# Insert the initialization right after initialize_app_paths() but before subprocess.run
RUN sed -i '/^initialize_app_paths()$/a\
\
# Initialize Agent Studio directories and symlinks at runtime\
subprocess.run(["/usr/local/bin/init-agent-studio-dirs.sh"], check=True)' /studio_app/startup_scripts/run-app.py

# Switch to cdsw user for runtime (Cloudera AI runs as this user)
USER cdsw
WORKDIR /home/cdsw